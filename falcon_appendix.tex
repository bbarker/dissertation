\section{Installing FALCON}

\hl {It is empty.}

\section{Running FALCON}

There are several ways to run FALCON. The script that implements
Algorithm~\ref{alg:FALCON} is \textit{falcon.m}; this script takes
many optional values, and the function signature is highly likely
to change in the near future. For basic use, we suggest using
the wrapper script \textit{runFalcon.m}, which assumes several
parameter values. This wrapper could be easily changed to use
different parameter defaults, or to call \textit{falconMulti.m},
itself a wrapper with a similar function signature for
\textit{falcon.m} that calculates the mean and standard deviation of
fluxes across multiple calls to \textit{falcon.m}.

\section{Assumptions for enzyme complex formation}
\label{sec:complexation}

In order to quantify enzyme complex formation (sometimes called enzyme
complexation), the notion of an enzyme complex should be formalized.
A protein complex typically refers to two or more physically
associated polypeptide chains, which is sometimes called a quaternary
structure. Since we are not exclusively dealing with multiprotein
complexes, we refer to an enzyme complex as being one or more
polypeptide chains that act together to carry out metabolic
catalysis.

\emph{Assumption~\ref{asm:expcorr}.}  The first assumption that we
need in order to guarantee an accurate estimate of (relative) enzyme
complex copy number are accurate measurements of their component
subunits. Unfortunately, this is currently not possible, and we almost
always must make do with mRNA measurements, which may even have some
degree of inaccuracy in measuring the mRNA copy number. What has been
seen is that Spearman's $\rho = 0.6$ for correlation between RNA-Seq
and protein intensity in datasets from HeLa cells
\citep{Nagaraj2011}. This implies that much can likely still be
gleamed from analyzing RNA-Seq data, but, an appropriate degree of
caution must be used in interpreting results based on RNA-Seq data. By
incorporating more information, such as metabolic constraints, we hope
to obviate some of the error in estimating protein intensity from
RNA-Seq data. 

\emph{Assumption~\ref{asm:isozyme}.} We also include the notion of
isozymes--different proteins that catalyze the same reaction--in our
notion of enzyme complex. Isozymes may arise by having one or more
differing protein isoforms, and even though these isoforms may not be
present in the same complex at the same moment, we consider them to be
part of the enzyme complex since one could be substituted for the
other.

As an example for assumptions described so far, take the $F_1$
subcomplex of ATP Synthase (Figure ~\ref{fig:2F43}), which is composed
of seven protein subunits (distinguished by color, left). On the
right-hand side we see different isoforms depicted as different
colors.  Error in expression data aside, instead of considering the
copy numbers with multiplicity and dividing their expression values by
their multiplicity, it may be easier to simply note that the axle
peptide (shown in red in the center of the complex) only has one copy
in the complex, so its expression should be an overall good estimation
of the $F_1$ subcomplex copy number. This reasoning will be useful
later in considering why GPR rules may be largely adequate for estimating
the abundance of most enzyme complexes.

\begin{figure*}%[H]
\label{fig:2F43}
\centering
\includegraphics[clip=true,trim=0cm 0cm 0cm 0cm, width=12cm]{2F43}
\caption{Illustration of the $F_1$ part of the ATP Synthase complex
  (PDB ID 1E79; \citealt{Gibbons2000,Bernstein1978,Gezelter}).
  This illustration demonstrates both how an enzyme complex may be
  constituted by multiple subunits (left), and how some of those
  subunits may be products of the same gene and have differing
  stoichiometries within the complex (right).}
\end{figure*}

\emph{Assumption~\ref{asm:hierarchy}.}
The modeling of enzyme complex copy number can be tackled by using
nested sets of subcomplexes; each enzyme complex consists of multiple
subcomplexes, unless it is only a single protein or family of protein
isozymes.  These subcomplexes are required for the enzyme complex to
function (AND relationships), and can be thought of as the division of
the complex in to distinct units that each have some necessary
function for the complex, with the exception that we do not keep track
of the multiplicity of subcomplexes within a complex since this
information is, in the current state of affairs, not always known.
However, there may be alternative versions of each functional set
(given by OR relationships). Eventually, this nested embedding
terminates with a single protein or set of peptide isoforms
(e.g.\ isozymes).  In the case of ATP Synthase, one of its functional
sets is represented by the $F_1$ subcomplex. The $F_1$ subcomplex
itself can be viewed as having two immediate subcomplexes: the single
$\gamma$ (axle) subunit and three identical subcomplexes each made of
an $\alpha$ and $\beta$ subunit. Each $\alpha\beta$ pair works
together to bind ADP and catalyze the reaction \citep{Oster2003}. The
$\alpha\beta$ subcomplex itself then has two subcomplexes composed of
just an $\alpha$ subunit on the one hand and the $\beta$ subunit on
the other.  It is obvious that one of these base-level functional
subcomplexes (in this example, either $\gamma$ or $\alpha\beta$) will
be in most limited supply, and that it will best represent the overall
enzyme complex copy number (discounting the issues of multiplicity for
$\alpha\beta$, discussed above).

%
% Consider adding this as a Theorem/Proof:
%

The hierarchical structure just described, when written out in
Boolean, will give a rule in CNF (conjunctive normal form), or more
specifically (owing to the lack of negations), clausal normal form,
where a clause is a disjunction of literals (genes). This is because all
relations are ANDs (conjunctions), except possibly at the inner-most
subcomplexes that have alternative isoforms, which are expressed as
ORs (disjunctions). Since GPR rules alone only specify the
requirements for enzyme complex formation, we will see that not all
forms of Boolean rules are equally useful in evaluating the enzyme
complex copy number, but we have established the assumptions in
Table~\ref{tab:ECAssume} and an alternative and logically equivalent rule
\citep{Russell2009} under which we can estimate enzyme complex copy
number.

\begin{table}
\def \ECAssumeCap {Assumptions in GPR-based Enzyme Complex Formation}
\ifthenelse{\boolean{thesisStyle}}{
  \begin{center} % also adds a little needed vspace
  \begin{tabular}{| p{0.9\textwidth} |}
  \hline
  \textbf{Table ~\ref{tab:ECAssume}. \ECAssumeCap} \\
  \hline
  \input{falcon_assumptions}
  \\ \hline
  \end{tabular}
  \end{center}

} {
  % For Bioinformatics:
  %\begin{table*}[!t]
  \processtable{\ECAssumeCap \label{tab:ECAssume}}{
  \begin{tabular}{| p{\textwidth} |}
  \hline
  \input{falcon_assumptions}
  \\ \hline
  \end{tabular}
  }
  {} % caption
}
\caption{\hl{it is empty}} %Needed a caption for correct numbering?!?
\label{tab:ECAssume}
\end{table}

There is no guarantee that a GPR rule has been written down with this
hierarchical structure in mind, though it is likely the case much of
the time as it is a natural way to model complexes.  However, any GPR
rule can be interpreted in the context of this hierarchical view due
to the existence of a logically equivalent CNF rule for any non-CNF
rule, and it is obvious that logical equivalence is all that is
required to check for enzyme complex formation when exact isoform
stoichiometry is unknown.  As an example, we consider another common
formulation for GPR rules, and a way to think about enzyme
structure---disjunctive normal form (DNF).  A DNF rule is a
disjunctive list of conjunctions of peptide isoforms, where each
conjunction is some variation of the enzyme complex due to
substituting in different isoforms for some of the required
subunits. A rule with a more complicated structure and compatible
isoforms across subcomplexes may be written more succinctly in CNF,
whereas a rule with only very few alternatives derived from isoform
variants may be represented clearly with DNF.  In rare cases, it is
possible that a GPR rule is written in neither DNF or CNF, perhaps
because neither of these two alternatives above are strictly the case,
and some other rule is more succinct.

\emph{Assumptions~\ref{asm:nostoich},~\ref{asm:sharing}~and~\ref{asm:active_site}.}
One active site per enzyme complex implies a single complex can only
catalyze one reaction at a time. Multimeric complexes with one active
site per identical subunit would be considered as one enzyme complex
per subunit in this model.  Note that it is possible for an enzyme
complex to catalyze different reactions. In fact, some transporter
complexes can transfer many different metabolites \hl{across a lipid
  bilayer---up to X in the case of Y}. Another example is the ligation or hydrolysis of
nucleotide, fatty acid, or peptide chains, where chains of different
length may all be substrates or products of the same enzyme
complex. While we do not explicitly consider these in
Algorithm~\ref{alg:ReductionToCNF}, these redundancies are taken into
account subsequently in Algorithm~\ref{alg:FALCON}.

What is currently not considered in our process is that some peptide
isoforms may find use in completely different complexes, and in some
cases, individual peptides may have multiple active sites; in the
first case, we assume an unrealistic case of superposition where the
isoform can simultaneously function in more than one complex. The
primary reason we have not tackled this problem is because exact
subunit stoichiometry of most enzyme complexes is not accurately
known, but an increasing abundance of data on BRENDA
\citep{Schomburg2013} gives some hope to this problem. A recent
\textit{E. coli} metabolic model incorporating the metabolism of all
known gene products \citep{O'Brien2013} also includes putative
enzyme complex stoichiometry in GPR rules. For the second point, there
are a few enzymes where a single polypeptide may have multiple active
sites (e.g.\ fatty acid synthase), and this is not currently taken into
account in our model. 

\emph{Assumption~\ref{asm:holo}.}
We do not make any special assumptions requiring symmetry of an
isoform within a complex. For instance, the example in
assumption~\ref{asm:holo} shows how you might have one subcomponent
composed of a single isoform, and another subcomponent composed of
that gene in addition to another isoform. In this case, it is simply
reduced to being the first gene only that is required, since clearly
the second is strictly optional. That isn't to say that the second
gene may not have some effect, such as (potentially) aiding in
structural ability or altering the catalytic rate, but it should have
no bearing on the formation of a functional catalytic
complex. Holoenzymes---enzymes with metabolic cofactors or protein
subunits that have a regulatory function for the complex---would
likely be the only situation where this type of rule might need to be
considered in more detail. But in the absence of detailed kinetic
information, this consideration (much like allosteric
regulation) is not useful.

\emph{Assumption~\ref{asm:enzyme_sensitivity}.}
Another important biochemical assumption is that reactions should
operate in a regime where they are sensitive to changes in the overall
enzyme level in the pathways that they belong in
\citep{Bennett2009,Chubukov2013}. This is perhaps the most important
issue to be explored further for methods like this, since if it is not
true, some other adjustment factor would be needed to make the method
realistic. For instance, if all reactions in a pathway are operating
far below $V_{max}$, but it is not the case in another pathway, the
current method does not have information on this, and will try to put
more flux through the first pathway than should be the case.

\emph{Assumptions~\ref{asm:chap}~and~\ref{asm:rate}.}
Due to the quickness, stability, and energetic favorability of enzyme
complex formation, the absence of chaperones or coupled metabolic
reactions required for complex formation may be reasonable
assumptions, but further research is warranted \citep{Karr2012}.
Additionally, as in metabolism, we assume a steady state for complex
formation, so that rate laws regarding complex formation aren't
needed. However, further research may be warranted to investigate the
use of a penalty for complex levels based on mass action and
protein-docking information. Requisite to this would be addressing
assumption~\ref{asm:nostoich}. It would be surprising (but not
impossible) if such a penalty were very large due to the cost this
would imply for many of the large and important enzyme complexes
present in all organisms \citep{Nelson2008}.

\section{The min-disjunction heuristic algorithm}
\label{sec:HeuristicToCNF}
Because conversion to CNF is potentially computationally intractable 
for some rules due to an  exponential increase in memory \citep{Russell2009}, we
present below a reduction rule that makes use of expression data.
The algorithm can be described as follows:

% TODO: write a function op commpand for M
\begin{AlgFloat}[H]
\begin{Algorithm}[heuristic min disjunction]
\label{alg:HeuristicToCNF}
\begin{algorithmic}
\ifthenelse{\boolean{thesisStyle}}{\singlespacing}{}
~
\INPUT $G = \left\{g_i~\mid~i \in{1, \ldots, m}\right\}$ are genes. 
\INPUT $r$ := A Boolean rule without negation consisting of\\
  $\left\{x_i~\mid~i \in{1, \ldots, n}\right\}$ Boolean sub-expressions.
\While{$rule \neq o_1 \land \ldots \land o_p$ where each $o_i$ is a disjunction
  of genes} 
  \If {Encounter a disjunction $x_d$ of conjunctions of genes}
  \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{
    Create sets from conjunctions, i.e.:\\
    \hspace{4.8 mm} Create $G_1$ and $G_2$ with $g_{1,i} \in G_1$ 
                    and $g_{2,i} \in G_2$ where\\ 
    \hspace{4.8 mm} $x_d = (g_{1,1} \land \ldots \land g_{1,r}) \lor 
                     (g_{2,1} \land \ldots \land g_{2,s})$. 
    \strut}
    \If {$G_1 \subseteq G_2$}
    \State Replace $x_d$ with $(g_{1,1} \land \ldots \land g_{1,r})$.
    \ElsIf {$G_2 \subseteq G_1$}
    \State Replace $x_d$ with $(g_{2,1} \land \ldots \land g_{2,s})$.
    \Else
    \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Create sets from conjunctions, i.e.:\\
      \hspace{4.8 mm} Create $G_1$ and $G_2$ with $g_{1,i} \in G_1$ 
                      and $g_{2,i} \in G_2$ where\\ 
      \hspace{4.8 mm} $x_d = (g_{1,1} \land \ldots \land g_{1,r}) \lor 
                       (g_{2,1} \land \ldots \land g_{2,s})$. \\
      \hspace{4.8 mm} Create $G_C = G_1 \cap G_2$, \\
      \hspace{4.8 mm} $G_{1\setminus C} = G_1 \setminus G_C$ and\\ 
      \hspace{4.8 mm} $G_{2\setminus C} = G_2 \setminus G_C$. 
      \strut}
      \If {$G_{1\setminus C} = \varnothing\: ||\: G_{2\setminus C} = \varnothing$}
        \State Substitute $x_d$ with $\ANDw_{g_c \in G_C} g_c$.
      \Else
        \State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Substitute $x_d$ with:\\
        \hspace{4.8 mm} $\ANDw_{g_c \in G_C} g_c \land (\mathcal{M}\left(G_{1 \setminus C}\right) \lor 
                         \mathcal{M}\left(G_{2\setminus C}\right)$ where\\
        \hspace{4.8 mm} $\mathcal{M}\left(S\right) = \argmin_{g \in S}{\E{S}}$
        \strut}
      \EndIf
    \EndIf
  \EndIf
  \State Distribute $\lor$ over $\land$, e.g.: $(x_1 \land x_2) 
          \lor (x_3 \land x_4)$ \\ 
    \hspace{4.8 mm} $\rightarrow (x_1 \lor x_3) \land (x_1 \lor x_4) \land 
          (x_2 \lor x_3) \land (x_2 \lor x_4)$
\EndWhile
\OUTPUT $o_{\min}$ where $o_{\min}$ has the form: $\ORw_{g_i \in G} g_i$
\end{algorithmic} 
\end{Algorithm}
\end{AlgFloat}

The intuition for this algorithm is that it returns the minimum
disjunction because at each iteration, we select the literal with
smallest value in a conjunction and remove all other literals in the
conjunction; distributing $\lor$ over $\land$ and subsequently
evaluating the associated expression values will not change which
disjunction attains the minimum value.

While this algorithm should work in most cases just as the algorithm
in Section~\ref{alg:ReductionToCNF}, there is a notable exception that
can occur when one or more genes appear in two conjunctions; this
possibility is handled in the conditional by treating the intersection
of the gene sets $G_C$ separately. Unfortunately, this step is not
associative over disjunctions, so if there are more than two
disjunctions at the same level, it is not guaranteed to be accurate.

While this algorithm is not currently implemented exactly as stated, 
an earlier version of this algorithm was implemented in ATS1, and
is available in the deprecated file \texttt{minDisjNoCov.dats}.
The algorithm can be implemented without much troubled based on the
existing code in the FALCON software package.

\subsection{Proof for the minDisjunction heristic algorithm}

Although the intuition presented above may be straightforward, we show
here in more verbosity that Algorithm~\ref{alg:HeuristicToCNF} works
as described.

\begin{Theorem}
\label{thm:HeuristicToCNF}
Algorithm~\ref{alg:HeuristicToCNF} returns the disjunction with
minimum expression value among all disjunctions of a rule in CNF.
\end{Theorem}

\begin{proof}
Due to our Boolean algebra currently lacking negation, the only step
in converting to CNF that we need is the distributive expansion of 
$\land$ over $\lor$. However, in order to allow $\mathcal{M}$ to be
applied to conjunctive sets that are in disjunction with one another, 
we must ensure that the sets are disjoint, which is taken care of first
in the algorithm by handling the intersection of the conjunctive sets, 
$G_C = G_1 \cap G_2$, separately. In practice, if $G_{1\setminus C} = \varnothing$
or $G_{2\setminus C} = \varnothing$, then this means we have a situation similar
to that described in assumption~\ref{asm:holo}, and the entire expression
is simplified to $\ANDw_{g_c \in G_C} g_c$. Otherwise, a direct route is 
taken to compute distributive law for the literals: 

$\ANDw_{g_c \in G_C} g_c \land \left(\ANDw_{g_i \in G_{1 \setminus C}, 
g_j \in G_{2\setminus C}} \left(g_i \lor g_j\right) \right)$

Below, we show that by applying $\mathcal{M}$ to $G_{1\setminus C}$
and $G_{2\setminus C}$, we can compute the correct minimum
disjunction without the need to compute and store in memory every 
disjunction in the expansion, which results in the following:

$\ANDw_{g_c \in G_C} g_c \land \left(\mathcal{M}(G_{1 \setminus C}) 
\lor \mathcal{M}\left(G_{2\setminus C}\right)\right)$

Now let us consider when expressions $x_1, ..., x_4$
are all distinct literals, and that
$\E{x_i}$ denotes the enzyme abundance for expression $x_i$ (or the 
expression measure of $x_i$ if $x_i$ is a gene). Assume
WLOG that $x_1 \lor x_3$ attains the minimum expression among the
disjunctions. Then we have:

\begin{align*}
&\E{x_1} + \E{x_3} \leq \E{x_1} + \E{x_4} \Rightarrow \E{x_3} \leq \E{x_4} \\
&\E{x_1} + \E{x_3} \leq \E{x_2} + \E{x_3} \Rightarrow \E{x_1} \leq \E{x_2} 
\end{align*}

Applying this result in conjunction to the application of $\mathcal{M}$ to
the original expression, $(x_1 \land x_2) \lor (x_3 \land x_4)$, we
immediately arrive at $(x_1) \lor (x_3)$, which gives our originally
assumed minimum. To show that this result doesn't depend on the $x_i$
being literals, merely consider repeating this process recursively for
each $x_i$ that is not a literal to arrive at two different
evaluations for $\E{x_i}$ (one where each evaluation is done with
reduction, and one where we evaluate entirely without reduction).
Since the process cannot continue indefinitely, eventually there is a
base case involving only literals, and the above result shows that, at
each step, as we backtrack from the base case, both evaluations will
be identical. The desired result is obtained because
Algorithm~\ref{alg:HeuristicToCNF} without the conditional simply yields CNF,
and it follows that adding the conditional will yield the disjunction with
minimum expression value of the rule in CNF.
\end{proof}

\section{Code excerpt for minimum disjunction algorithm}
\label{sec:code}

\hl{Need to update code here; try using atslistings.sty for code 
highlights.}

The following code is written in ATS, a type-safe language including
syntax similar to SML while having direct access to C types
\citep{ATStypes03}. \texttt{GREXP} describes a datatype that is used
for storing the parse trees of Boolean rules (without negation).  A
\texttt{GREXP} can then be manipulated by the function \texttt{toCNF}
to be converted to a conjunctive normal form. We make use of the
reduction rule mentioned previously by calling the \texttt{minConj}
function. The \texttt{conjunctivize} function is a helper function to
deal with different structures for \texttt{GRconj} and \texttt{GRdisj}
(one is set based, one is parse-tree based; note this described in the
data(view)type definition). We note that this is a recursive,
functional implementation of Algorithm~\ref{alg:ReductionToCNF}, which
seems more straightforward than a procedural implementation.

\begin{verbatim}
dataviewtype GREXP = 
  | GRgenes of genes
  | GRconj of genes
  | GRconj of (GREXP,GREXP)
  | GRdisj of genes
  | GRdisj of (GREXP,GREXP)

extern
fun toCNF (bexp: GREXP, emap: &gDMap): GREXP

implement
toCNF (bexp, emap): GREXP = let     
  val LR:GREXP = (case+ bexp of 
    | ~GRconj(ex1,ex2) => GRconj (toCNF(ex1,emap),toCNF(ex2,emap))
    | ~GRdisj(ex1,ex2) => GRdisj (toCNF(ex1,emap),toCNF(ex2,emap))   
    | GR => GR):GREXP
  in (case+ LR of  
    | ~GRconj(ex1,ex2) => minConj(GRconj(ex1,ex2),emap) 
    | ~GRdisj(ex1,ex2) => (case+ (ex1,ex2) of
      // Handle disjunctive leaf cases:         
      | (~GRdisj(lx), ~GRgenes(g)) => GRdisj (lx + g) 
      | (~GRgenes(g), ~GRdisj(rx)) => GRdisj (rx + g)
      | (~GRdisj(lx), ~GRdisj(rx)) => GRdisj (lx + rx)
      | (~GRgenes(g1), ~GRgenes(g2)) => GRdisj (g1 + g2)

      // Distribute OR over ANDs:
      | (~GRconj(x1,x2), ~GRconj(g)) => conj1(x1,x2,g,emap) 
      | (~GRconj(g), ~GRconj(x1,x2)) => conj1(x1,x2,g,emap) 
      | (~GRconj(g1), ~GRconj(g2)) => conj2(g1,g2,emap)
      | (~GRconj(lx1,lx2), ~GRconj(rx1,rx2)) => let
        val lx1c = GREXP_copy(lx1)
        val lx2c = GREXP_copy(lx2) 
        val rx1c = GREXP_copy(rx1)
        val rx2c = GREXP_copy(rx2) 
        in GRconj(GRconj(GRconj(toCNF(GRdisj(lx1,rx1),emap), 
          toCNF(GRdisj (lx2, rx1c ),emap)),
          toCNF(GRdisj (lx1c, rx2),emap)), 
          toCNF(GRdisj (lx2c, rx2c),emap)) 
        end

      // Handle e.g.: (.. OR ..) OR (.. AND ...) 
      | (~GRconj(lx1,lx2), RX) => let
        val RXc = GREXP_copy(RX)
        in GRconj(toCNF(GRdisj(lx1,RX),emap),
          toCNF(GRdisj(lx2,RXc),emap)) 
        end
      | (LX ,~GRconj(rx1,rx2)) => let
        val LXc = GREXP_copy(LX)
        in GRconj(toCNF(GRdisj(LX,rx1),emap),
          toCNF(GRdisj(LXc,rx2),emap)) 
        end
      | (~GRconj(gc), RX) => let
        val retGR = toCNF(conjunctivize(RX, gc,emap),emap)
        val _ = genes_free(gc)
        in retGR end
      | (LX, ~GRconj(gc)) => let
        val retGR = toCNF (conjunctivize(LX, gc,emap),emap)
        val _ = genes_free(gc)
        in retGR end
      // All other disjunctive cases
      | (_,_) => GRdisj(toCNF(ex1,emap),toCNF(ex2,emap))
      ):GREXP
    | EX => EX
    ):GREXP
  end
\end{verbatim}

\subsection{Parsing functions and other GPR tools}
\label{ssec:parsing}

\hl{Need to include examples on how to construct a GREXP, how to call
parser on de novo or IO-based GPR rule.}

\section{FALCON internals and experimental features}
\label{sec:internals}

\subsection{EXPCON: expression constraints for $V_{max}$}
An experimental feature that has been implemented is to constrain
enzymatic fluxes to be strictly less than or equal to the
automatically scaled enzyme abundance. For example, in the
notation of Algorithm~\ref{alg:FALCON}, for each reversible $v_j$
associated to enzyme complex $i$, we would have the additional
constraints:

\[ v_{j,f} \leq n e_i \]
\[ v_{j,b} \leq n e_i \]

This feature may be employed by setting the \texttt{EXPCON} parameter
to \texttt{true} when calling \texttt{falcon}. We implemented this
feature because we have seen and heard of others seeing promising
results when na\"ively using scaled expression to act as a surrogate for
$V_{max}$ in standard FBA (for an abstraction and example, see
\citealt{Colijn2009}). In the present study, we observed similar
results when using this parameter, suggesting that---at least for the
yeast models---this parameter may provide additional benefit when
appended to the existing FALCON machinery. We speculate that larger
and less constrained models may benefit from employing this parameter.

\subsection{Regularization}
Although sometimes called by different names, regularization is an
optimization technique that alters a problem so that there is some
penalty for variables taking on large values, where large may be
defined in different ways. In our case, since we are using a linear
programming problem, it is most convenient to use the $L_1$-norm to
assess how large a penalty should be given. We employ this as an
optional parameter to \texttt{falcon}, \texttt{rc} that is applied to
all flux values.  Regularization has been found to be a biologically
important objective in microbes \citep{Schuetz2012}, but we didn't
find any significant differences when using it in the present study.

\subsection{Growth rates}
Sometimes a growth rate may be known or we may wish to simulate a
growth rate. If we have a high-confidence biomass pseudo-reaction in
our model, the optional \texttt{minFit} parameter to \texttt{falcon}
can be used to set the minimum growth rate. Since biomass is a
complicated sink in the model, it is apparently unlikely that the
FALCON objective will direct flux into biomass directly, so other
measures of growth rate may also be warranted, such as accounting
for individual sink reactions. This part of \texttt{falcon} could
easily be modified to support more reactions beyond biomass.

\subsection{Debugging and developing falcon}
The \texttt{FDEBUG} parameter to \texttt{falcon} can be used to
display additional information while the FALCON algorithm is
running. As this can potentially be quite verbose, \texttt{FDEBUG} is
normally set to \texttt{false}. \texttt{FDEBUG} is also a parameter to
\texttt{computeMinDisj}, the MATLAB wrapper function for the
\texttt{minDisj} executable, and is used similarly there.


\subsection{Including reversible reactions in the FALCON objective}
Since FALCON has been designed for use with irreversible models, there
is no mathematical problem with including both the forward and
backward reactions of a reversible reaction in the FALCON objective,
but this may give undesired results in some cases, such as cycles
between the forward and backward reactions (the cycles are not a
problem directly because they can be removed: see the functions
\texttt{setFBRxnDirection} in \texttt{falcon.m}.  These cycles can
then give rise to objective values that may be quite high simply due
to having many cycles.

%\subsection{Linear Fractional Program (Charnes-Cooper) Transformation}
%Since our denominator is simply the variable $n$, it is always
%required to be equal to 1 in the transformed problem. However,
%this is scaled by $z$, which may vary. To be sure that $z$ never
%goes to zero, we set a lower about on $z$ with the \texttt{ZMIN}
%variable. We know $z = 0$ is undesirable in our case since
%we have $z = \frac{1}{n}$ and $v_original = n v_{LFP} = \frac{v_{LFP}}{z}$.
%!! Actually, we should not need this, since dividing by small z would make for 
% a very large original objective value. This may only be needed in cases
% where we aren't using a true LFP.

\subsection{LP solver settings}
\label{ssec:lpsettings}
We have exclusively used the Gurobi solver \citep{gurobi} for this
work, which is a highly competitive solver that employs by default a
parallel strategy to solving problems: a different algorithm is run
simultaneously, and as soon as one algorithm finished the others
terminate. Of course, if there is a clear choice of algorithm for a
particular problem class, this should be used in production settings
to avoid wasted CPU time and memory. In order to address this, we
benchmarked the three non-parallel solver methods in Gurobi
 (since parallel solvers simply use multiple methods simultaneously).
The exception to this rule is the Barrier method, which can use
multiple threads, but in practice for our models appears to use
no more than about 6 full CPU cores simultaneously for our models.
Our results for Yeast 5 and Yeast 7 with minimal directionality constraints
\citep{Heavner2012,Lee2012,Aung2013} and Human Recon 2 \citep{Thiele2013}
are shown in Table~\ref{tab:methodTime}).

\begin{table}
\begin{center}
\begin{tabular}{rrrr}
\emph{Model}                 & \emph{Primal-Simplex} & \emph{Dual-Simplex} & \emph{Barrier} \\
Yeast 5.21 (2,061 reactions) & $ 7.841 \pm 1.697    $ & $ 7.611 \pm 1.267    $ & $ 10.859 \pm 2.788   $\\ 
Yeast 7.0 (3,498 reactions)  & $ 51.863 \pm 22.731  $ & $ 65.317 \pm 12.771  $ & $ 242.137 \pm 57.129 $\\
Human 2.03 (7,440 reactions) & $ 159.077 \pm 24.903 $ & $ 152.297 \pm 39.783 $ & $ 366.166 \pm 92.321 $\\
\end{tabular}
\end{center}
\caption{Running times (in seconds, $\pm$ standard deviation) for
  FALCON using various algorithms implemented in the Gurobi package.
  For yeast models, 1,000 replicates were performed, and for the human
  model, 100 replicates were performed.}
\label{tab:methodTime}
\end{table}

We found that in Yeast7 with the primal-simplex solver, there is a
chance the solver will fail to find a feasible solution.
We verified that this is a numeric issue
in Gurobi and can be fixed by setting the Gurobi parameter
\texttt{MarkowitzTol} to a larger value (which decreases
time-efficiency but limits the numerical error in the
simplex algorithm). In practice, failure for the algorithm to converge
at an advanced iteration is rare and is not always a major problem (since the previous
flux estimate by the advanced iteration should already be quite good), but it
is certainly undesirable; a warning message will be printed by
\texttt{falcon} if this occurs, at which point parameter settings can
be investigated. In the future, we plan to improve \texttt{falcon} so
that parameters will be adjusted as needed during progression of the
algorithm after finding a good test suite of models and data. For now,
we use the dual-simplex solver, for which we have always had good
results.

Because the number of iterations depends non-trivially on the model
and the expression data, it may be more helpful to look at the 
average time per iteration in the above examples (Table~\ref{tab:methodTimeIter}).

\begin{table}
\begin{center}
\begin{tabular}{rrrr}
\emph{Model}                 & \emph{Primal-Simplex} & \emph{Dual-Simplex} & \emph{Barrier} \\
Yeast 5.21 (2,061 reactions) & $ 0.721 \pm 0.023 $ & $ 0.652 \pm 0.040 $ & $ 1.100 \pm 0.112  $\\ 
Yeast 7.0 (3,498 reactions)  & $ 2.725 \pm 0.298 $ & $ 2.469 \pm 0.289 $ & $ 11.309 \pm 1.589 $\\
Human 2.03 (7,440 reactions) & $ 6.422 \pm 0.484 $ & $ 5.233 \pm 0.661 $ & $ 15.782 \pm 3.209 $\\ 
\end{tabular}
\end{center}
\caption{Running time per FALCON iteration (in seconds, $\pm$ standard
  deviation) using various algorithms implemented in the Gurobi
  package.  For yeast models, 1,000 replicates were performed, and for
  the human model, 100 replicates were performed.}
\label{tab:methodTimeIter}
\end{table}

Given the above rare trouble with primal simplex solver the universal
best performance enjoyed by the dual-simplex method (Table~\ref{tab:methodTime},
Table~\ref{tab:methodTimeIter}), we would advise the dual-simplex algorithms, all else
being equal. The dual-simplex method is also recommended for
memory-efficiency by Gurobi documentation, but we did not observe any
differences in memory for different solver methods.

It is not possible to pass the Gurobi \texttt{method} parameter to
\texttt{gurobi} in the COBRA Toolbox by default, but to keep our code
as close as possible to the COBRA Toolbox API, we have copied
\texttt{solveCobraLP.m} to the file \texttt{solveFalconLP.m} and
modified it to use the optimal method parameters. Other parameters
could also be passed easily. \texttt{solveFalconLP} is only called if
\texttt{gurobi} is detected as a MATLAB executable (MEX) file;
otherwise the default solver specified for the COBRA Toolbox will be
used. At the time of this release, \texttt{solveFalconLP} should be
completely compatible with all other possible calls made to
\texttt{solveCobraLP}, so in principle the latter could be replaced,
but it is not recommended in case the COBRA Toolbox API changes. If a
user wishes to use some solver other than Gurobi, it should be easy to
alter \texttt{solveFalconLP.m} and \texttt{falcon.m} to support custom
parameters. 

\section{Generation of figures and tables}

All non-trivial figures can be generated using MATLAB scripts found in
the \texttt{analysis/figures} subdirectory of the FALCON installation.
In particular, figures should be generated through the master script
\texttt{makeMethodFigures.m} by calling
\texttt{makeMethodFigures(figName)} where \texttt{figName} has a name
corresponding to the desired figure.  In some cases, some MATLAB
\texttt{.mat} files will need to be generated by other scripts first;
see the plotting scripts or the subsections below for details. An
example is to make the scatter plots showing the difference between
running falcon with enzyme abundances determined by direct evaluation
or the minimum disjunction algorithm; all three scatter plots are
generated with the command \texttt{makeMethodFigures(\textquotesingle
fluxCmpScatter\textquotesingle)}. Note that, as written, this requires
a graphical MATLAB session.

\subsection{Timing Analyses}
All timing analyses were performed on a system with four 8-core AMD
Opteron\texttrademark\ 6136 processors operating at 2.4
GHz. Figure FluxBars, and Tables FalcPerf, \ref{tab:methodTime} and 
\ref{tab:methodTimeIter} used a single
unperturbed expression file per species (\textit{S. cerevisiae},
\textit{E. coli}, and \textit{H. sapiens}; see
\texttt{timingAnalysis.m} for details). Values for the FALCON method
were averaged across 32 replicates, while values for the
\citealt{Lee2012} method were averaged across 8 replicates. \suppOrApp
Tables~\ref{tab:methodTime} and \ref{tab:methodTimeIter} used
log-normal noise applied to the original expression vector (refer to
Section~\ref{sec:noise} for details) to introduce more variance in the
calculations; the human models were tested with 100 replicates and the
yeast models with 500 replicates.

\subsection{Expression perturbation analysis}
\label{sec:noise}
\hl{Describe log-normal noise sampling here first.}
