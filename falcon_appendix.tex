
\section{Proof for the minDisjunction algorithm}

Although the logic in the main text may be straightforward, we show
here in more verbosity that Algorithm~\ref{alg:ReductionToCNF} works
as described.

\begin{Theorem}
\label{thm:ReductionToCNF}
Algorithm~\ref{alg:ReductionToCNF} returns the disjunction with
minimum expression value among all disjunctions of a rule in CNF.
\end{Theorem}

\begin{proof}
The third step in the while-loop does not affect the
underlying logic, so we only need to consider the effect of step one
on step two.  Let us first consider when expressions $x_1, ..., x_4$
are all literals in step two to cover the most complex case, and that
$x_i^{(e)}$ denotes the expression measurement for gene $x_i$. Assume
WLOG that $x_1 \lor x_3$ attains the minimum expression among the
disjunctions. Then we have:

\begin{align*}
&x_{1}^{(e)} + x_{3}^{(e)} \leq x_{1}^{(e)} + x_{4}^{(e)} \Rightarrow x_{3}^{(e)} \leq x_{4}^{(e)} \\
&x_{1}^{(e)} + x_{3}^{(e)} \leq x_{2}^{(e)} + x_{3}^{(e)} \Rightarrow x_{1}^{(e)} \leq x_{2}^{(e)} 
\end{align*}

Applying this result in conjunction to step one in the while-loop to
the original expression, $(x_1 \land x_2) \lor (x_3 \land x_4)$, we
immediately arrive at $(x_1) \lor (x_3)$, which gives our originally
assumed minimum. To show that this result doesn't depend on the $x_i$
being literals, merely consider repeating this process recursively for
each $x_i$ that is not a literal to arrive at two different
evaluations for $x_i^{(e)}$ (one where each evaluation is done with reduction, 
and one where we evaluate entirely without reduction). 
Since the process cannot continue
indefinitely, eventually there is a base case involving only literals,
and the above result shows that, at each step, as we backtrack from the base case,
both evaluations will be identical. The desired result is obtained
because Algorithm~\ref{alg:ReductionToCNF} without step one simply yields CNF,
and it follows that adding step one will yield the disjunction with 
minimum expression value of the rule in CNF.
\end{proof}

\section{Code excerpt for minimum disjunction algorithm}
\label{sec:code}

The following code is written in ATS, a type-safe language including
syntax similar to SML while having direct access to C types
\citep{ATStypes03}. GREXP describes a datatype that is used for
storing the parse trees of Boolean rules (without negation).  A GREXP
can then be manipulated by the function toCNF to be converted to a
conjunctive normal form. We make use of the reduction rule mentioned
previously by calling the minConj function. The conjunctivize function
is a helper function to deal with different structures for a GRconj
and GRdisj (one is set based, one is parse-tree based; note this
described in the data(view)type defintion). We note that this is a
recursive, functional implementation of
Algorithm~\ref{alg:ReductionToCNF}, which seems more straightforward
than a procedural implementation.

\begin{verbatim}
dataviewtype GREXP = 
  | GRgenes of genes
  | GRconj of genes
  | GRconj of (GREXP,GREXP)
  | GRdisj of genes
  | GRdisj of (GREXP,GREXP)

extern
fun toCNF (bexp: GREXP, emap: &gDMap): GREXP

implement
toCNF (bexp, emap): GREXP = let     
  val LR:GREXP = (case+ bexp of 
    | ~GRconj(ex1,ex2) => GRconj (toCNF(ex1,emap),toCNF(ex2,emap))
    | ~GRdisj(ex1,ex2) => GRdisj (toCNF(ex1,emap),toCNF(ex2,emap))   
    | GR => GR):GREXP
  in (case+ LR of  
    | ~GRconj(ex1,ex2) => minConj(GRconj(ex1,ex2),emap) 
    | ~GRdisj(ex1,ex2) => (case+ (ex1,ex2) of
      // Handle disjunctive leaf cases:         
      | (~GRdisj(lx), ~GRgenes(g)) => GRdisj (lx + g) 
      | (~GRgenes(g), ~GRdisj(rx)) => GRdisj (rx + g)
      | (~GRdisj(lx), ~GRdisj(rx)) => GRdisj (lx + rx)
      | (~GRgenes(g1), ~GRgenes(g2)) => GRdisj (g1 + g2)

      // Distribute OR over ANDs:
      | (~GRconj(x1,x2), ~GRconj(g)) => conj1(x1,x2,g,emap) 
      | (~GRconj(g), ~GRconj(x1,x2)) => conj1(x1,x2,g,emap) 
      | (~GRconj(g1), ~GRconj(g2)) => conj2(g1,g2,emap)
      | (~GRconj(lx1,lx2), ~GRconj(rx1,rx2)) => let
        val lx1c = GREXP_copy(lx1)
        val lx2c = GREXP_copy(lx2) 
        val rx1c = GREXP_copy(rx1)
        val rx2c = GREXP_copy(rx2) 
        in GRconj(GRconj(GRconj(toCNF(GRdisj(lx1,rx1),emap), 
          toCNF(GRdisj (lx2, rx1c ),emap)),
          toCNF(GRdisj (lx1c, rx2),emap)), 
          toCNF(GRdisj (lx2c, rx2c),emap)) 
        end

      // Handle e.g.: (.. OR ..) OR (.. AND ...) 
      | (~GRconj(lx1,lx2), RX) => let
        val RXc = GREXP_copy(RX)
        in GRconj(toCNF(GRdisj(lx1,RX),emap),
          toCNF(GRdisj(lx2,RXc),emap)) 
        end
      | (LX ,~GRconj(rx1,rx2)) => let
        val LXc = GREXP_copy(LX)
        in GRconj(toCNF(GRdisj(LX,rx1),emap),
          toCNF(GRdisj(LXc,rx2),emap)) 
        end
      | (~GRconj(gc), RX) => let
        val retGR = toCNF(conjunctivize(RX, gc,emap),emap)
        val _ = genes_free(gc)
        in retGR end
      | (LX, ~GRconj(gc)) => let
        val retGR = toCNF (conjunctivize(LX, gc,emap),emap)
        val _ = genes_free(gc)
        in retGR end
      // All other disjunctive cases
      | (_,_) => GRdisj(toCNF(ex1,emap),toCNF(ex2,emap))
      ):GREXP
    | EX => EX
    ):GREXP
  end
\end{verbatim}

\section{FALCON internals and experimental features}
\label{sec:internals}

\subsection{EXPCON: expression constraints for $V_{max}$}
An experimental feature that has been implemented is to constrain
enzymatic fluxes to be strictly less than or equal to the
automatically scaled enzyme intensity level. For example, in the
notation of Algorithm~\ref{alg:FALCON}, for each reversible $v_j$
associated to enzyme complex $i$, we would have the additional
constraints:

\[ v_{j,f} \leq n e_i \]
\[ v_{j,b} \leq n e_i \]

This feature may be employed by setting the \texttt{EXPCON} parameter
to \texttt{true} when calling \texttt{falcon}. We implemented this
feature because we have seen and heard of others seeing promising
results when na\"ively using scaled expression to act as a surrogate for
$V_{max}$ in standard FBA. In the present study, we observed similar
results when using this parameter, suggesting that---at least for the
yeast models---this parameter may provide additional benefit when
appended to the existing FALCON machinery. We speculate that larger
and less constrained models may benefit from employing this parameter.

\subsection{Regularization}
Although sometimes called by different names, regularization is an
optimization technique that alters a problem so that there is some
penalty for variables taking on large values, where large may be
defined in different ways. In our case, since we are using a linear
programming problem, it is most convenient to use the $L_1$-norm to
assess how large a penalty should be given. We employ this as an
optional parameter to \texttt{falcon}, \texttt{rc} that is applied to
all flux values.  Regularization has been found to be a biologically
important objective in microbes \citep{Schuetz2012}, but we didn't
find any significant differences when using it in the present study.

\subsection{Growth rates}
Sometimes a growth rate may be known or we may wish to simulate a
growth rate. If we have a high-confidence biomass pseudo-reaction in
our model, the optional \texttt{minFit} parameter to \texttt{falcon}
can be used to set the minimum growth rate. Since biomass is a
complicated sink in the model, it is apparentl unlikely that the
FALCON objective will direct flux into biomass directly, so other
measures of growth rate may also be warranted, such as accounting
for individual sink reactions. This part of \texttt{falcon} could
easily be modified to support more reactions beyond biomass.

\subsection{Debugging and developing falcon}
The \texttt{FDEBUG} parameter to \texttt{falcon} can be used to
display additional information while the FALCON algorithm is
running. As this can potentially be quite verbose, \texttt{FDEBUG} is
normally set to \texttt{false}. \texttt{FDEBUG} is also a parameter to
\texttt{computeMinDisj}, the MATLAB wrapper function for the
\texttt{minDisj} executable, and is used similarly there.


\subsection{Including reversible reactions in the FALCON objective}
Since FALCON has been designed for use with irreversible models, there
is no mathematical problem with including both the forward and
backward reactions of a reversible reaction in the FALCON objective,
but this may give undesired results in some cases, such as cycles
between the forward and backward reactions (the cycles are not a
problem directly because they can be removed: see the functions
\texttt{setFBRxnDirection} in \texttt{falcom.m}.  These cycles can
then give rise to objective values that may be quite high simply due
to having many cycles.

\subsection{LP solver settings}
We have exclusively used the Gurobi solver \citep{gurobi} for this
work, which is a highly competitive solver that employs by default a
parallel strategy to solving problems: a different algorithm is run
simultaneously, and as soon as one algorithm finished the others
terminate. Of course, if there is a clear choice of algorithm for a
particular problem class, this should be used in production settings
to avoid wasted CPU time and memory.

\section{Generation of figures and tables}

\subsection{Minimally constrained yeast models}
The MATLAB function \texttt{removeEnzymeIrrevs} was written to find
all enzymatic reactions in a model that are annotated as reversible
but are constrained to operate in one direction only. The script then
changes the bounds to allow flux in both directions. The fuction
\texttt{useYN5irrevs} copies the irreversible annotations found in
Yeast 5.21 \citep{Lee2012} to a newer yeast model, but could in
principle be used for any two models; this script is coded to first
call \texttt{removeEnzymeIrrevs} on both models before copying
irreversible annotations.
